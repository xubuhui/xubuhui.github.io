{"meta":{"title":"HeZ","subtitle":null,"description":"纵有千古，横有八荒；前途似海，来日方长。","author":"HeZ","url":"https://xubuhui.github.io"},"pages":[{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2019-07-18T05:52:03.778Z","comments":false,"path":"about/index.html","permalink":"https://xubuhui.github.io/about/index.html","excerpt":"","text":"[HeZ] 与&nbsp; HeZ （ 悔やまないことを許す ） 对话中... function bot_ui_ini() { var botui = new BotUI(\"hello-mashiro\"); botui.message.add({ delay: 800, content: \"Hi！ 👋\" }).then(function () { botui.message.add({ delay: 1100, content: \"这里是 HeZ\" }).then(function () { botui.message.add({ delay: 1100, content: \"一个可爱的蓝孩子~\" }).then(function () { botui.action.button({ delay: 1600, action: [{ text: \"然后呢？ 😃\", value: \"sure\" }, { text: \"少废话！ 🙄\", value: \"skip\" }] }).then(function (a) { \"sure\" == a.value && sure(); \"skip\" == a.value && end() }) }) }) }); var sure = function () { botui.message.add({ delay: 600, content: \"😘\" }).then(function () { secondpart() }) }, end = function () { botui.message.add({ delay: 600, content: \"![...](https://view.moezx.cc/images/2018/05/06/a1c4cd0452528b572af37952489372b6.md.jpg)\" }) }, secondpart = function () { botui.message.add({ delay: 1500, content: \"目前就读于山东某所神秘学校\" }).then(function () { botui.message.add({ delay: 1500, content: \"向往技术，也喜欢游戏\" }).then(function () { botui.message.add({ delay: 1200, content: \"偶尔会变成中二少年\" }).then(function () { botui.message.add({ delay: 1500, content: \"主攻 php 语言和 Python，偶尔也折腾 HTML/CSS/JavaScript/Java\" }).then(function () { botui.message.add({ delay: 1500, content: \"毕业想去深圳发展，实现王总的小目标\" }).then(function () { botui.message.add({ delay: 1800, content: \"喜欢lol,年轻时候想过去打职业，现在...只能玩玩匹配这样子了\" }).then(function () { botui.action.button({ delay: 1100, action: [{ text: \"为什么叫HeZ呢？ 🤔\", value: \"why-HeZ\" }] }).then(function (a) { thirdpart() }) }) }) }) }) }) }) }, thirdpart = function () { botui.message.add({ delay: 1E3, content: \"HeZ...emmmmm，频率单位嘛，主要是觉得叫起来酷酷的~\" }).then(function () { botui.action.button({ delay: 1500, action: [{ text: \"为什么是许不悔呢？ 🤔\", value: \"why-Buhui Xu\" }] }).then(function (a) { fourthpart() }) }) }, fourthpart = function () { botui.message.add({ delay: 1E3, content: \"因为希望自己做的每一个决定都不后悔 \" }).then(function () { botui.message.add({ delay: 1100, content: \"而且我希望自己以后能直面生活，勇往直前\" }).then(function () { botui.action.button({ delay: 1500, action: [{ text: \"域名有什么含意吗？(ง •_•)ง\", value: \"why-domain\" }] }).then(function (a) { fifthpart() }) }) }) }, fifthpart = function () { botui.message.add({ delay: 1E3, content: \"域名还没加上呢，慢慢的我会加上的。\" }).then(function () { botui.message.add({ delay: 1600, content: \"那么，仔细看看我的博客吧？ ^_^\" }) }) } } bot_ui_ini()","keywords":"关于"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"comment/index.html","permalink":"https://xubuhui.github.io/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"client/index.html","permalink":"https://xubuhui.github.io/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2019-07-18T08:41:33.713Z","comments":false,"path":"lab/index.html","permalink":"https://xubuhui.github.io/lab/index.html","excerpt":"","text":"sakura主题 谢谢Mashiro大神辛苦制作的主题！","keywords":"Lab实验室"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2019-07-18T05:50:46.250Z","comments":false,"path":"donate/index.html","permalink":"https://xubuhui.github.io/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了ya~"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"rss/index.html","permalink":"https://xubuhui.github.io/rss/index.html","excerpt":"","text":""},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2019-07-20T09:31:47.398Z","comments":false,"path":"music/index.html","permalink":"https://xubuhui.github.io/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"tags/index.html","permalink":"https://xubuhui.github.io/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://xubuhui.github.io/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"bangumi/index.html","permalink":"https://xubuhui.github.io/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"links","date":"2019-08-05T12:11:06.000Z","updated":"2019-08-07T02:32:28.433Z","comments":true,"path":"links/index.html","permalink":"https://xubuhui.github.io/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"video/index.html","permalink":"https://xubuhui.github.io/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"python播放vip视频+打包成exe","slug":"python播放vip视频+打包成exe","date":"2020-04-29T15:42:30.000Z","updated":"2020-04-29T15:46:28.190Z","comments":true,"path":"2020/04/29/python播放vip视频+打包成exe/","link":"","permalink":"https://xubuhui.github.io/2020/04/29/python播放vip视频+打包成exe/","excerpt":"","text":"💌实现思路 1.爬取解析网址解析视频url 2.分析视频解析流程，拼接最终解析地址 3.制作gui界面 ​ 请自动忽略界面🙄 4.打包成exe可执行文件 这里我使用的是pyinstaller。 安装pyinstaller： pip install pyinstaller 打包exe方式，直接在.py文件同目录下cmd执行： pyinstaller vipvideo.py 这是最简单的打包方式。 .exe文件在根目录下dist/vipvideo下 🎡代码实现 import requests from lxml import etree import tkinter as tk import webbrowser # 画板 root = tk.Tk() root.title('HeZ电影') # 标题 root.geometry('550x280') # 长宽 字母x表示乘 gui = tk.Label(root, text='方法:复制vip电影地址到链接栏，选中一个接口点击播放', font=(&quot;宋体&quot;, 8)) gui.grid(row=1, column=0) gui1 = tk.Label(root, text='播放接口', font=(&quot;Arial&quot;, 12)) gui1.grid(row=3, column=0) gui2 = tk.Label(root, text='播放链接', font=(&quot;Arial&quot;, 12)) gui2.grid(row=6, column=0) t1 = tk.Entry(root, text='', width=50) t1.grid(row=7, column=0) def get_url(): # 解析地址 url = 'http://www.qmaile.com/' response = requests.get(url) # 提取解析url html = etree.HTML(response.text) target = html.xpath('//option/@value') # 有一个重复的url，去重 target = list(set(target)) return target # 单选按钮 var = tk.StringVar() # 默认选中 var.set(get_url()[0]) for i in range(len(get_url())): tk.Radiobutton(root, text='播放接口%s' % (i + 1),variable=var, value=get_url()[i]).grid(row=i, column=1) def del_text(): # 从头删到尾 t1.delete(0, 'end') def bf(): webbrowser.open(var.get() + t1.get()) # webbrowser.open() # 播放 btn_play = tk.Button(root, text='播放', font=(&quot;Arial&quot;, 12), width=6, command=bf) btn_play.grid(row=7, column=1) # 清除 btn_del = tk.Button(root, text='清除', font=(&quot;Arial&quot;, 12), width=6, command=del_text) # command调用函数名 btn_del.grid(row=7, column=2) # 消息循环 root.mainloop() 🕶总结 程序运行比较慢。 代码没优化，界面就不提了。 接口如果卡顿，可以切换一个试试。 仅供个人学习参考。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取IP代理","slug":"python爬取IP代理","date":"2020-04-29T15:42:30.000Z","updated":"2020-04-30T07:00:39.193Z","comments":true,"path":"2020/04/29/python爬取IP代理/","link":"","permalink":"https://xubuhui.github.io/2020/04/29/python爬取IP代理/","excerpt":"","text":"💌实现思路","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"github+picgo+typora搭建图床","slug":"github+picgo+typora搭建图床","date":"2020-04-29T04:00:00.000Z","updated":"2020-04-29T06:13:31.842Z","comments":true,"path":"2020/04/29/github+picgo+typora搭建图床/","link":"","permalink":"https://xubuhui.github.io/2020/04/29/github+picgo+typora搭建图床/","excerpt":"","text":"🎈前言 这篇blog之前我的图片，基本都是截图先push到github，然后更新版本号，利用jsdelivr的图片url显示在bolg。 真的是又费劲又费时（我都懒得多加截图了）。 当你get到这个小技巧之后，保证你的markdown写blog事半功倍，以后截图想发多少发多少，MM再也不用担心我更新截图费时了emmm。 参考 官方配置文档 🍬准备工作 🍪github 1.注册github github地址。 2.创建一个新项目，用来存储图片。 3.生成一个token用于PicGo操作你的仓库： 访问：https://github.com/settings/tokens 然后点击Generate new token。 把repo的勾打上即可。然后翻到页面最底部，点击Generate token的绿色按钮生成token。 **注意：**这个token生成后只会显示一次！你要把这个token复制一下存到其他地方以备以后要用。 🍖Picgo 1.下载地址 选择下载类型： 2.解压安装到非英文目录 3.配置picgo 仓库名——github名/项目仓库名 分支——默认主分支master即可 Token——在github中保存的token 存储路径——项目下的子目录 重命名——在上传图片的时候可以给图片重命名（默认时间命名） 别的就不一一介绍了。 🍩Typora 1.下载安装最好用的markdown编辑工具 typora官网 2.配置typora 文件-&gt;偏好设置 根据我图中配置即可： 上传服务： 选择PicGo（app） PicGo路径：选择你安装的PicGo下的exe文件 🍭开始工作 1.截图粘帖到typora中 2.右击图片——&gt;上传图片——&gt;重命名——&gt;上传成功 3.可以在picgo和github仓库中看到刚上传的图片 😰踩坑记录 1.验证失败可能因为你已经验证成功并把两张图上传到github图床了 解决：请去github仓库删除 typora-icon.png typora-icon2.png两张图 并重新检测 2. 出现 Failed to fetch 解决：确认在PicGo中打开了Server服务，监听端口与typora检验端口保持一致 3.就是不行？？？ 出现 Failed to fetch 解决：确认在PicGo中打开了Server服务，监听端口与typora检验端口保持一致 3.就是不行？？？ 有可能是代理的问题，关闭代理，重启电脑。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"typora","slug":"typora","permalink":"https://xubuhui.github.io/tags/typora/"},{"name":"picgo","slug":"picgo","permalink":"https://xubuhui.github.io/tags/picgo/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"hexo快速安装插件","slug":"hexo快速安装插件","date":"2020-04-28T16:20:30.000Z","updated":"2020-04-28T16:49:59.167Z","comments":true,"path":"2020/04/29/hexo快速安装插件/","link":"","permalink":"https://xubuhui.github.io/2020/04/29/hexo快速安装插件/","excerpt":"","text":"🎈前言 准备给hexo扩展几个插件，一开始用npm的方式安装，速度慢的令人咋舌， 于是准备找一种方式给hexo安装插件的时候提速。 几经周折，发现使用淘宝的cnpm镜像会大大提升安装插件的速度。 🎄使用方式 1.首先安装淘宝镜像 $ npm install -g cnpm --registry=https://registry.npm.taobao.org 2.安装模块 $ cnpm install [name] 其中 [name] 是我们要安装的插件名 , npm改为cnpm，其他参数不变。 💦例如 $ cnpm install hexo-deployer-ftpsync –save 💨注意 淘宝镜像为阿里云产物，服务器架设在国内，当cnpm上没有你需要的插件时，它会自动去npm服务器上寻找同时在保存至cnpm服务器上。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://xubuhui.github.io/tags/hexo/"},{"name":"插件","slug":"插件","permalink":"https://xubuhui.github.io/tags/插件/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取数据之多线程","slug":"python爬取数据之多线程","date":"2020-04-28T05:00:00.000Z","updated":"2020-04-28T07:14:38.864Z","comments":true,"path":"2020/04/28/python爬取数据之多线程/","link":"","permalink":"https://xubuhui.github.io/2020/04/28/python爬取数据之多线程/","excerpt":"","text":"前言 这一篇我们利用多线程从猫眼电影Top100爬取数据。 多线程可以大大的提高我们爬虫的爬取速度，类似的提速方式有 多线程 进程 协程 准备工作 需要引入的包 import requests import threading import parsel import time import openpyxl 网页分析 其中每一部电影的信息都存在dd标签内，循环遍历dd标签可以得到对应电影的具体信息。 控制分页的参数是offset。 其中第一页是offset=0，第二页是offset=10…第10页是offset=90。 for i in range(0, 100, 10) 代码实现 import requests import threading import parsel import time import openpyxl def url_get(ws, page): url = 'https://maoyan.com/board/4?offset=' + str(page) headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36&quot; } response = requests.get(url, headers=headers) sel = parsel.Selector(response.text) # css选择器 先取dd标签 dds = sel.css('dd') # 遍历dd标签 for dd in dds: name_sel = dd.css('p.name a::text').getall()[0] # strip去空格 star_sel = dd.css('p.star::text').getall()[0].strip() time_sel = dd.css('p.releasetime::text').getall()[0] # 拼接 score_sel = ''.join(dd.css('p.score i::text').getall()) data = [ name_sel, star_sel, time_sel, score_sel ] ws.append(data) def url_start(): start_time = time.time() # 创建工作簿 wb = openpyxl.Workbook() # 创建工作表 ws = wb.active ws.title = '猫眼数据' # 表头 ws.append(['电影', '主演', '上映时间', '评分']) for i in range(0, 100, 10): # 多线程，同时请求十页 # target目标方法 args元祖参数 如果一个元祖需要在后边加 , threading.Thread(target=url_get, args=(ws, i)).start() # url_get(ws, i) # 线程数11 因为有一个主线程 print('线程数{}'.format(len(threading.enumerate()))) # 计算真实消耗时间 while len(threading.enumerate()) &gt; 1: pass wb.save('cat_data.xlsx') end_time = time.time() print(&quot;消耗时间:%s秒&quot; % (end_time - start_time)) url_start()","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"正则表达式","slug":"正则表达式","date":"2020-04-27T15:55:30.000Z","updated":"2020-04-29T03:56:39.843Z","comments":true,"path":"2020/04/27/正则表达式/","link":"","permalink":"https://xubuhui.github.io/2020/04/27/正则表达式/","excerpt":"","text":"re正则表达式 作用 匹配字符串 从字符串中获取想要的部分 对目标字符串进行替换 正则表达式的使用 🍺在python中使用，需要引入re模块 import re 😇匹配字符 符号 作用 ** 转义符，改变原来符号含义，后面会有演示 [ ] 中括号用来创建一个字符集，第一个出现字符如果是^，表示反向匹配 😀预定义字符集 符号 作用 \\d 匹配数字，如：[0-9] \\D 与上面正好相反，匹配所有非数字字符。 \\s 空白字符，如：空格，\\t\\r\\n\\f\\v等。 \\S 非空白字符。 \\w 单词字符，如：大写A~Z，小写a~z，数字0~9。 \\W 非\\w字符。 😃可选项与重复子模式 符号 作用 ***** 匹配前一个字符0次或无限次数。 + 匹配前一个字符1次或无限次数。 ? 匹配前一个字符0次或1次。 {m} 匹配前一个字符m次。 {m,n} 匹配前一个字符m至n次。 未完","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://xubuhui.github.io/tags/正则表达式/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取图片（二）-海贼王高清壁纸","slug":"python爬取图片（二）-海贼王高清壁纸","date":"2020-04-26T04:16:30.000Z","updated":"2020-04-26T07:53:53.187Z","comments":true,"path":"2020/04/26/python爬取图片（二）-海贼王高清壁纸/","link":"","permalink":"https://xubuhui.github.io/2020/04/26/python爬取图片（二）-海贼王高清壁纸/","excerpt":"","text":"网页分析 海贼王高清壁纸 代码我们主要是用的etree模块获取网页元素 谷歌浏览器，f12检查发现 目标后缀路径获取 //ul[@class=“pic-list2 clearfix”]//a[@class=“pic”]/@href 在Elements栏直接ctrl+f输入上列代码可以定位到符合位置 这说明图片这里的图片只是一个展示性的图片，点进图片后进入图片集 继续分析，发现每一个图片都包裹在这里 图片路径获取 //img[@id=“bigImg”]/@src 下一张图片路径获取 ​ //a[@id=“pageNext”]/@href 当然路径获取的是一个列表，如果需要获取字符串，直接取下标为0的元素 具体代码实现 import requests from lxml import etree # 图片下载 def download_img(img_url, headers, num): img_content = requests.get(img_url, headers).content # 文件路径名 img_url[-15:]截取字符串 # 提前在项目根目录建好onePiece_img文件夹 file_name = 'onePiece_img/' + str(num) + '_' + img_url[-15:] with open(file_name, 'wb') as img: img.write(img_content) print(&quot;正在下载%s&quot; % file_name) # 图片解析 def parses_img(targets_url, headers, num, page): url = 'http://desk.zol.com.cn/' + targets_url response = requests.get(url, headers) html = etree.HTML(response.text) # 捕获异常 try: # list提取 img_url = html.xpath('//img[@id=&quot;bigImg&quot;]/@src')[0] download_img(img_url, headers, num) img_next = html.xpath('//a[@id=&quot;pageNext&quot;]/@href')[0] parses_img(img_next, headers, num, page) except: print(&quot;第%s页第%s组结束&quot; % (page, num)) # 获取url def get_url(): # 爬两页的图片 for i in range(2): if i == 0: url = 'http://desk.zol.com.cn/dongman/haizeiwang/' else: url = 'http://desk.zol.com.cn/dongman/haizeiwang/' + str(i + 1) + '.html' page = i + 1 # 浏览器标识 headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&quot; } response = requests.get(url, headers) html = etree.HTML(response.text) targets_url = html.xpath('//ul[@class=&quot;pic-list2 clearfix&quot;]//a[@class=&quot;pic&quot;]/@href') num = 1 for url in targets_url: parses_img(url, headers, num, page) num = num + 1 get_url()","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取百度疫情数据(三) - pyecharts疫情分布地图","slug":"python爬取百度疫情数据(三) - pyecharts疫情分布地图","date":"2020-04-25T10:15:30.000Z","updated":"2020-04-26T07:52:15.126Z","comments":true,"path":"2020/04/25/python爬取百度疫情数据(三) - pyecharts疫情分布地图/","link":"","permalink":"https://xubuhui.github.io/2020/04/25/python爬取百度疫情数据(三) - pyecharts疫情分布地图/","excerpt":"","text":"前言 这一篇综合从百度实时疫情网址爬取数据并用pyecharts显示疫情地图分布。 其中地图分为具体省份疫情地图和中国总体疫情地图。 每一个省份对应一个html地图。 pyecharts map模块链接 实现思路是 获取数据 保存为html.txt文件 解析html.txt，获取需要的数据，并保存为data.json 分别创建省份疫情地图和中国疫情的方法，填充数据 运行main.py 遇到的问题 在map_draw.py中，其中具体省份疫情地图是用的pyecharts官网广东地图实例，其中有一个参数Faker.guangdong_city，我们把demo复制到pycharm ctrl+鼠标左键 进入这个参数后， 发现这个参数的数据是这样的： guangdong_city = [“汕头市”, “汕尾市”, “揭阳市”, “阳江市”, “肇庆市”, “广州市”, “惠州市”] 而我们获取的数据有些是不带市的，所以要进行一下字符串的处理 for each_city in each['subList']: city.append(each_city['city'] + &quot;市&quot;) confirmeds.append(each_city['confirmed']) map.to_map_city(city, confirmeds, province, update_tqime) if province == '上海' or '北京' or '天津' or '重庆': for each_city in each['subList']: city.append(each_city['city']) confirmeds.append(each_city['confirmed']) map.to_map_city(city, confirmeds, province, update_time) 注意需要提前在根目录建好map文件夹，存储具体省份的疫情图。 最终效果 中国总体累计确诊分布图： 广东省累计确诊分布图： 具体代码实现 data_get.py: import requests from lxml import etree import json import re # 获取数据 class Get_data(): # 抓取数据 def get_data(self): url = 'https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_1' response = requests.get(url) with open('html.txt','w') as file: file.write(response.text) print(&quot;写入txt成功&quot;) # 获得更新时间 def get_time(self): with open('html.txt', 'r') as file: text = file.read() time = re.findall('&quot;mapLastUpdatedTime&quot;:&quot;(.*?)&quot;', text)[0] return time # 解析数据 def parse_data(self): with open('html.txt', 'r') as file: text = file.read() html = etree.HTML(text) result = html.xpath('//script[@type=&quot;application/json&quot;]/text()')[0] result = json.loads(result) result = result['component'][0]['caseList'] # result['component'][0]获得列表第0项 是一个字典 # print(result['component'][0]['caseList']) # 字典转换为字符串 result = json.dumps(result) with open('data.json', 'w') as file: file.write(result) print('写入json成功') # data = Get_data() # data.get_data() # data.get_time() # data.parse_data() data_more.py import json import map_draw import data_get with open('data.json', 'r') as file: data = file.read() data = json.loads(data) map = map_draw.Draw_map() # print(data) # 中国疫情地图数据 def china_map(update_time): area = [] confirmed = [] for each in data: # print(each) area.append(each['area']) confirmed.append(each['confirmed']) map.to_map_china(area, confirmed, update_time) # 省份疫情地图数据 def province_map(update_time): for each in data: city = [] confirmeds = [] province = each['area'] # 出现空列表是因为特别行政区 for each_city in each['subList']: city.append(each_city['city'] + &quot;市&quot;) confirmeds.append(each_city['confirmed']) map.to_map_city(city, confirmeds, province, update_time) if province == '上海' or '北京' or '天津' or '重庆': for each_city in each['subList']: city.append(each_city['city']) confirmeds.append(each_city['confirmed']) map.to_map_city(city, confirmeds, province, update_time) print(&quot;生成城市疫情地图成功&quot;) map_draw.py from pyecharts import options as opts from pyecharts.charts import Map from pyecharts.faker import Faker class Draw_map(): # 颜色RGB转换 脱离函数，不需要定义self def get_color(self,a,b,c): result = '#'+''.join(map((lambda x:&quot;%02x&quot; % x),(a,b,c))) return result.upper() def to_map_city(self, area, variate,province,update_time): pieces = [ {&quot;max&quot;: 99999999, &quot;min&quot;: 1001, 'label': '&gt;10000', 'color': self.get_color(102,2,8)}, {&quot;max&quot;: 9999, &quot;min&quot;: 1000, 'label': '1000-9999', 'color':self.get_color(140,13,13)}, {&quot;max&quot;: 999, &quot;min&quot;: 500, 'label': '500-999', 'color':self.get_color(204,41,41)}, {&quot;max&quot;: 499, &quot;min&quot;: 100, 'label': '100-999', 'color': self.get_color(255,123,105)}, {&quot;max&quot;: 99, &quot;min&quot;: 50, 'label': '50-99', 'color': self.get_color(255,170,133)}, {&quot;max&quot;: 49, &quot;min&quot;: 10, 'label': '10-49', 'color':self.get_color(255,202,179)}, {&quot;max&quot;: 9, &quot;min&quot;: 1, 'label': '1-9', 'color': self.get_color(255,228,217)}, {&quot;max&quot;: 0, &quot;min&quot;: 0, 'label': '0', 'color': self.get_color(255,255,255)}, ] c = ( # 设置地图大小 Map(init_opts=opts.InitOpts(width='1000px',height='880px')) .add(&quot;累计确诊人数&quot;, [list(z) for z in zip(area, variate)],province,is_map_symbol_show=False) .set_global_opts( title_opts=opts.TitleOpts(title=&quot;%s地区疫情地图分布&quot;%(province),subtitle='截至%s %s省疫情分布情况'%(update_time,province) ,pos_left='center',pos_top='30px',), legend_opts=opts.LegendOpts(is_show=False), visualmap_opts=opts.VisualMapOpts(max_=200, is_piecewise=True, pieces=pieces) ) .render(&quot;./map/{}疫情地图.html&quot;.format(province)) ) def to_map_china(self, area, variate, update_time): # 分段 pieces = [ {&quot;max&quot;: 99999999, &quot;min&quot;: 1001, 'label': '&gt;10000', 'color': '#8A0808'}, {&quot;max&quot;: 9999, &quot;min&quot;: 1000, 'label': '1000-9999', 'color': '#B40404'}, {&quot;max&quot;: 999, &quot;min&quot;: 100, 'label': '100-999', 'color': '#DF0101'}, {&quot;max&quot;: 99, &quot;min&quot;: 10, 'label': '1-9', 'color': '#F5A9A9'}, {&quot;max&quot;: 9, &quot;min&quot;: 1, 'label': '1-9', 'color': '#F5A9A9'}, {&quot;max&quot;: 0, &quot;min&quot;: 0, 'label': '0', 'color': '#FFFFFF'}, ] c = ( Map(init_opts=opts.InitOpts(width='1000px', height='880px')) # zip内置函数，实现数据对应 .add(&quot;累计确诊人数&quot;, [list(z) for z in zip(area, variate)], &quot;china&quot;) .set_global_opts( title_opts=opts.TitleOpts(title=&quot;中国疫情地图分布&quot;, subtitle='截至%s 中国疫情分布情况' % (update_time), pos_left='center',pos_top='30px'), visualmap_opts=opts.VisualMapOpts(max_=200, is_piecewise=True, pieces=pieces), ) .render(&quot;中国疫情地图.html&quot;) ) print(&quot;生成中国疫情地图成功&quot;) main.py import data_more import data_get datas = data_get.Get_data() # 爬取数据并保存 datas.get_data() # 更新时间 update_time = datas.get_time() # 解析数据 datas.parse_data() # 生成中国疫情地图 data_more.china_map(update_time) # 生成城市疫情地图 data_more.province_map(update_time)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取百度疫情数据(二) - wordcloud词云","slug":"python爬取百度疫情数据(二) - wordcloud词云","date":"2020-04-24T02:10:30.000Z","updated":"2020-04-28T05:17:35.112Z","comments":true,"path":"2020/04/24/python爬取百度疫情数据(二) - wordcloud词云/","link":"","permalink":"https://xubuhui.github.io/2020/04/24/python爬取百度疫情数据(二) - wordcloud词云/","excerpt":"","text":"前言 数据用的是上一篇我们从百度实时疫情数据扒下来的，具体数据获取方式见本blog上一篇。 我们主要获取中国和世界的累计确诊人数分别制作词云,词越大表示确诊人数越多. 准备工作 代码主要用到两个模块: openpyxl WordCloud wordcloud 库把词云当作一个WordCloud对象 wordcloud.WordCloud()代表一个文本对应的词云 可以根据文本中词语出现的频率等参数绘制词云 绘制词云的形状、尺寸和颜色均可设定 以WordCloud对象为基础，配置参数、加载文本、输出文件 参数 描述 width 指定词云对象生成图片的宽度,默认400像素 height 指定词云对象生成图片的高度,默认200像素 min_font_size 指定词云中字体的最小字号，默认4号 font_step 指定词云中字体字号的步进间隔，默认为1 font_path 指定文体文件的路径，默认None，解决中文乱码 max_words 指定词云显示的最大单词数量,默认200 stop_words 指定词云的排除词列表，即不显示的单词列表 mask 指定词云形状，默认为长方形，需要引用imread()函数 background_color 指定词云图片的背景颜色，默认为黑色 实现效果图 中国疫情词云 世界疫情词云 具体代码实现 import openpyxl from wordcloud import WordCloud # 读取数据 wb = openpyxl.load_workbook('data.xlsx') # 获取工作表 ws =wb['国内疫情'] # 国内疫情数据 frequency_in = {} for row in ws.values: # 去除表头 if row[0] == '省份': pass else: # 将省份和累计确诊数关联 float转换为浮点型 frequency_in[row[0]] = float(row[1]) # 海外疫情数据 frequency_out = {} # 获取表名 sheet_name = wb.sheetnames for each in sheet_name: if '洲' in each: ws = wb[each] for row in ws.values: if row[0] =='国家': pass else: # 写入字典 frequency_out[row[0]] = float(row[1]) # 创建生成词云方法 def generate_pic(frequency, name): # font_path 字体路径 解决不显示中文 wordcloud = WordCloud(font_path='msyh.ttc', width=1920, height=1080) # 根据确诊病例生成词云 wordcloud.generate_from_frequencies(frequency) wordcloud.to_file('%s.png'%(name)) print(&quot;ok!&quot;) generate_pic(frequency_in, &quot;国内疫情词云&quot;) generate_pic(frequency_out, &quot;世界疫情词云&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取百度疫情数据(一)","slug":"python爬取百度疫情数据(一)","date":"2020-04-23T12:10:30.000Z","updated":"2020-04-26T07:48:43.531Z","comments":true,"path":"2020/04/23/python爬取百度疫情数据(一)/","link":"","permalink":"https://xubuhui.github.io/2020/04/23/python爬取百度疫情数据(一)/","excerpt":"","text":"前言 百度疫情和腾讯疫情数据数据爬取方式不同，腾讯疫情数据把数据放在数据包里，而百度则把数据包裹在script标签内。 百度实时疫情网址 准备工作 编辑器还是推荐 PyCharm下载链接 本篇需要导入的python模块 import json import requests from lxml import etree import openpyxl json —— 老朋友了，这个模块主要是解码编码，以及格式转换 requests——灵活的爬虫库，requests请求的底层实现其实就是urllib3，不要再用urllib2做爬虫了，兄弟萌。 etree——主要是为了从html源码中得到自己想要的内容。 openpyxl——处理excel文件 抓包分析 我感觉做爬虫最重要的还是数据来源的分析，因为每一个网站数据来源都有可能不一样，就像百度和腾讯做的疫情实时数据，不得不说，百度这一块做得确实比腾讯有过之而无不及。 首先进入网站，我一开始也是老三样，F12-&gt;Network-&gt;XHR，发现只有一个数据包，但是并没有我们想要的数据。 然后准备从网页源码入手，复制一个数据查找后，发现数据被包裹在一个script标签内，然后准备先把script标签内的数据扒下来。 分析数据的时候要跟原网页数据对应，确定数据的中文意思。 国内数据在caseList内 国内数据对应参数 省份——area 累计确诊——confirmed 死亡——died 治愈——crued 现有确诊——curConfirm 累计确诊增量——confirmedRelative 海外数据在globalList内 海外数据对应参数 洲——area 国家——country 累计确诊——confirmed 死亡——died 治愈——crued 现有确诊——curConfirm 累计确诊增量——confirmedRelative 具体代码实现 import json import requests from lxml import etree import openpyxl url = 'https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_1' response = requests.get(url) # print(response.text) # 生成html对象 html = etree.HTML(response.text) # 获取列表 r = html.xpath('//script[@type=&quot;application/json&quot;]/text()') # json.loads将字符串转换为python数据类型 result = json.loads(r[0]) # print(result['component'][0]['caseList']) # 创建工作簿 wb = openpyxl.Workbook() # 创建工作表 ws = wb.active ws.title = '国内疫情' # 表头 ws.append(['省份','累计确诊','死亡','治愈','现有确诊','累计确诊增量']) # 国内疫情数据 result_in = result['component'][0]['caseList'] for data_in in result_in: temp_list = [data_in['area'], data_in['confirmed'],data_in['died'],data_in['crued'],data_in['curConfirm'], data_in['confirmedRelative'] ] # 判断是否为空 如果空数据则赋值 0，0是字符串 for i in range(len(temp_list)): if temp_list[i] == '': temp_list[i] = '0' ws.append(temp_list) # 国外疫情数据 result_out = result['component'][0]['globalList']; # print(result_out) for data_out in result_out: # 表名 sheet_title = data_out['area'] # 创建新的工作表 ws_out = wb.create_sheet(sheet_title) ws_out.append(['国家','累计确诊','死亡','治愈','现有确诊','累计确诊增量']) # subList 具体国家疫情数据 for sub_out in data_out['subList']: sub_list = [sub_out['country'], sub_out['confirmed'],sub_out['died'],sub_out['crued'],sub_out['curConfirm'], sub_out['confirmedRelative']] # 防止数据为空 如果空则 赋值0 for i in range(len(sub_list)): if sub_list[i] == '': sub_list[i] = '0' ws_out.append(sub_list) wb.save('./data.xlsx') print(&quot;Ok&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取腾讯疫情数据并可视化（二）","slug":"python爬取腾讯疫情数据并可视化（二）","date":"2020-04-22T13:50:30.000Z","updated":"2020-04-26T07:51:19.744Z","comments":true,"path":"2020/04/22/python爬取腾讯疫情数据并可视化（二）/","link":"","permalink":"https://xubuhui.github.io/2020/04/22/python爬取腾讯疫情数据并可视化（二）/","excerpt":"","text":"前言 我们需要爬取的是腾讯实时疫情的数据–腾讯实时疫情网址 这篇用pyecharts的南丁格尔(玫瑰图)实现数据可视化。 准备工作 本篇需要pyecharts库，安装方式如下： 例：安装方式-&gt;CMD命令行-&gt;安装requests pip install requests 这样安装的pyechatrs V1为最新版，与pyecharts–0.5不兼容 我们的数据为 最终实现效果 下面是具体代码实现 import pandas as pd from pyecharts.charts import Pie from pyecharts import options as opts # 读入数据, nrows数据行数 df = pd.read_csv('data.csv',nrows=30) # 降序操作 df.sort_values(by='确诊病例', ascending=False, inplace=True) # 提取数据 area = df['国家和地区'].values.tolist() num = df['确诊病例'].values.tolist() # 自定义颜色 color_series = [ '#D02C2A', '#D44C2D', '#F57A34', '#FA8F2F', '#D99D21', '#CF7B25', '#CF7B25', '#CF7B25','#FAE927', '#E9E416', '#C9DA36', '#9ECB3C', '#6DBC49', '#37B44E', '#3DBA78', '#14ADCF', '#209AC9', '#1E91CA', '#2C6BA0', '#2B55A1', '#2D3D8E', '#44388E', '#6A368B' '#7D3990', '#A63F98', '#C31C88', '#D52178', '#D5225B', ] # 实例化Pie类 pie1 = Pie(init_opts=opts.InitOpts(width='1350px', height='700px')) # 设置颜色 pie1.set_colors(color_series) # 添加数据，设置饼图的半径，是否展示成南丁格尔图 pie1.add(&quot;&quot;, [(a,b) for a,b in zip(area, num)], radius=[&quot;40%&quot;, &quot;100%&quot;], center=[&quot;50%&quot;, &quot;65%&quot;], rosetype=&quot;radius&quot; ) # 设置全局配置项 pie1.set_global_opts(title_opts=opts.TitleOpts(title='南丁格尔疫情图'), legend_opts=opts.LegendOpts(is_show=False), ) # 设置系列配置项 pie1.set_series_opts(label_opts=opts.LabelOpts(is_show=True, position=&quot;top&quot;, font_size=12, formatter=&quot;{b}:累计确诊{c}例&quot;, font_style=&quot;italic&quot;, font_weight=&quot;bold&quot;, font_family=&quot;Microsoft YaHei&quot; ), ) # 生成html文档 pie1.render('南丁格尔疫情图.html') print(&quot;构建成功&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取腾讯疫情数据并可视化（一）","slug":"python爬取腾讯疫情数据并可视化（一）","date":"2020-04-22T12:50:30.000Z","updated":"2020-04-26T07:51:29.476Z","comments":true,"path":"2020/04/22/python爬取腾讯疫情数据并可视化（一）/","link":"","permalink":"https://xubuhui.github.io/2020/04/22/python爬取腾讯疫情数据并可视化（一）/","excerpt":"","text":"前言 我们需要爬取的是腾讯实时疫情的数据–腾讯实时疫情网址 这里准备用两篇文章实现python爬取疫情数据，并结合pyecharts的南丁格尔(玫瑰图)实现数据可视化。 准备工作 本篇需要用到三个python库, 请自行安装 requests pandas json 例：安装方式-&gt;CMD命令行-&gt;安装requests pip install requests 抓包分析 我用的是谷歌浏览器，进入腾讯实时疫情的网址，然后F12-&gt;Network-&gt;XHR，重新刷新下页面发现 确定这就是数据包。 下面是具体代码实现 import requests import json import pandas as pd # 分析抓包 url = 'https://api.inews.qq.com/newsqa/v1/automation/foreign/country/ranklist' response = requests.get(url) content = json.loads(response.text) # json.loads 将json格式数据转换为字典 # print(content) df = pd.DataFrame(columns=['国家和地区','确诊病例','死亡病例','治愈病例']) # 字典类型读取到DataFrame for i in range(len(content['data'])): df.loc[i+1] = [ content['data'][i]['name'], content['data'][i]['confirm'], content['data'][i]['dead'], content['data'][i]['heal']] # 保存为xlsx格式文件 df.to_excel('data.xlsx',index=0, encoding='utf-8') print('爬取完毕')","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"python爬取图片（一）-王者荣耀高清皮肤图","slug":"python爬取图片（一）-王者荣耀高清皮肤图","date":"2020-04-21T12:52:25.000Z","updated":"2020-04-26T07:56:28.633Z","comments":true,"path":"2020/04/21/python爬取图片（一）-王者荣耀高清皮肤图/","link":"","permalink":"https://xubuhui.github.io/2020/04/21/python爬取图片（一）-王者荣耀高清皮肤图/","excerpt":"","text":"关于爬虫的一些思路 首先我们打开王者荣耀官网，进入英雄资料界面 python爬取腾讯疫情数据并可视化（二） 进入一个英雄的具体介绍页面 然后f12，打开浏览器调试窗口，首先找到皮肤图片的url 分析提供图片的数据包 NetWork-&gt;XHR 下面是具体代码实现 # 抓取王者荣耀皮肤 import requests import pprint import time # 程序开始时间 start_time = time.time() # 1.分析目标网页，确定爬取的url路径 url = 'https://pvp.qq.com/web201605/js/herolist.json' # 2.发送请求 --requests 模拟浏览器发送请求，获取相应数据 response = requests.get(url) data = response.json() # 格式化打印json # pprint.pprint(data) # 3.解析数据 --json模块:把json字符串转化成python可交互的数据类型 for data1 in data: cname = data1['cname'] # 英雄名 ename = data1['ename'] # 英雄ID try: skin_name = data1['skin_name'].split('|') # 皮肤名称 except Exception as e: print(e) # 构建皮肤数量循环 for skin_num in range(1, len(skin_name)+1): # 拼接图片url # 'http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/'+英雄ID值+'/'+英雄ID值+'-bigskin-'+皮肤序号+'.jpg' skin_url = 'http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/'+str(ename)+'/'+str(ename)+'-bigskin-'+str(skin_num)+'.jpg' img_data = requests.get(skin_url).content # 二进制数据提取 # 4.保存数据 --保存在目标文件夹 with open('img\\\\'+cname+'-'+skin_name[skin_num-1]+'.jpg','wb') as f: print('正在下载图片：', cname+'-'+skin_name[skin_num-1]) f.write(img_data) f.close() # 程序结束时间 end_time = time.time() # 打印程序运行花费时间 print('花费时间：',end_time-start_time)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.github.io/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"php开启redis扩展","slug":"php开启redis扩展","date":"2020-04-10T07:12:30.000Z","updated":"2020-04-26T07:47:21.210Z","comments":true,"path":"2020/04/10/php开启redis扩展/","link":"","permalink":"https://xubuhui.github.io/2020/04/10/php开启redis扩展/","excerpt":"","text":"1.安装redis github下载链接 2.测试redis 进入redis目录，双击运行 redis-server.exe 文件。 或者在redis安装目录，运行cmd命令行，执行 redis-server.exe redis.windows.conf 3.安装redis扩展 redis扩展下载链接 然后根据phpinfo()的信息选择合适的redis扩展压缩包。 注意php版本号以及 Architecture ：CPU架构 4.将redis扩展包的php_redis.dll和php_redis.pdb两个文件放在php的ext文件夹 5.打开php.ini文件，打开扩展 extension=php_redis.dll 6.php连接并测试redis数据库(先开启redis服务-redis-server) 首先新建test.php &lt;?php $redis = new Redis(); $redis-&gt;connect('127.0.0.1',6379); $redis-&gt;set('name','HeZ'); echo $redis-&gt;get('name'); 访问test.php,输出HeZ,测试通过。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"php","slug":"php","permalink":"https://xubuhui.github.io/tags/php/"},{"name":"redis","slug":"redis","permalink":"https://xubuhui.github.io/tags/redis/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"js上传并显示图片","slug":"js上传并显示图片","date":"2019-08-02T05:49:34.000Z","updated":"2019-09-24T00:34:45.308Z","comments":true,"path":"2019/08/02/js上传并显示图片/","link":"","permalink":"https://xubuhui.github.io/2019/08/02/js上传并显示图片/","excerpt":"","text":"首先看下upload.html文件 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;上传图片&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;css/up.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;divcss&quot;&gt; &lt;div class=&quot;portrait&quot;&gt; &lt;img src=&quot;img/avatar.png&quot; alt=&quot;&quot; class=&quot;avatar&quot; id=&quot;imgupload&quot;&gt; &lt;p&gt;头像&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;file&quot;&gt; 点击上传 &lt;input type=&quot;file&quot; id=&quot;profile_pic&quot; &gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;script src=&quot;https://code.jquery.com/jquery-3.1.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; //上传个人头像 $(&quot;#profile_pic&quot;).change(function (e) { console.log(e); var file = e.target.files[0] || e.dataTransfer.files[0]; if (file) { //............. console.log(file); var file = document.getElementById(&quot;profile_pic&quot;).files[0]; var formData = new FormData(); formData.append('file', file); //为ajax的data做准备 var reader = new FileReader(); reader.onload = function () { $(&quot;#imgupload&quot;).attr(&quot;src&quot;, this.result); }; reader.readAsDataURL(file); $.ajax({ url: url + &quot;api/uploadOss/uploadFile&quot;,//这是你的后端接口地址 type: &quot;post&quot;,//ajax请求类型 data: formData,//存放图片资源 contentType: false, processData: false, mimeType: &quot;multipart/form-data&quot;, success: function (data) { //成功返回信息 alert(&quot;上传成功&quot;); }, error: function (data) { //失败返回信息 alert(&quot;上传头像失败&quot;); } }); } }); &lt;/script&gt; &lt;/html&gt; 其次看下up.css文件 body {background-image:url('../img/point.png');} .divcss { margin: 0 auto; width: 400px; height: 100px; text-align: center } .avatar { width: 350px; height: 400px } .file { position: relative; display: inline-block; background: #D0EEFF; border: 1px solid #99D3F5; border-radius: 4px; padding: 4px 12px; overflow: hidden; color: #1E88C7; text-decoration: none; text-indent: 0; line-height: 20px; } .file input { position: absolute; font-size: 100px; right: 0; top: 0; opacity: 0; } .file:hover { background: #AADFFD; border-color: #78C3F3; color: #004974; text-decoration: none; } 其中有几处用到了图片背景，这个你可以根据喜好选择图片，当然也可以不用。 我的目录结构是这样子的： 最终的效果图是这样的： 码完收工。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://xubuhui.github.io/tags/web/"},{"name":"javaScript","slug":"javaScript","permalink":"https://xubuhui.github.io/tags/javaScript/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"超简单！js使用echarts","slug":"超简单！js使用echarts","date":"2019-07-24T04:27:54.000Z","updated":"2020-04-26T07:53:04.132Z","comments":true,"path":"2019/07/24/超简单！js使用echarts/","link":"","permalink":"https://xubuhui.github.io/2019/07/24/超简单！js使用echarts/","excerpt":"","text":"echarts Echarts的官网https://echarts.baidu.com/,Echarts官网有很多demo可以供我们使用，这一点可以说是很良心了，不说废话了直接进入主题。 第一步 首先在确定安装好node.js的前提下，在项目目录cmd命令行下输入 npm install echarts 安装echarts依赖。 第二步 把图中所示的echart.js文件复制到和你的项目文件同级目录下再引入,当然你也可以直接引入，我主要是为了方便。 第三步 新建html文件，输入代码 &lt;body&gt; &lt;!--创建div容器--&gt; &lt;div id=&quot;chart&quot; style=&quot;width:600px; height: 400px;&quot;&gt;&lt;/div&gt; &lt;/body&gt; &lt;!--引入charts.js--&gt; &lt;script src=&quot;echarts.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; //指定图标的配置和数据 var option = { title:{ text:'ECharts 数据统计' }, tooltip:{}, legend:{ data:['用户来源'] }, xAxis:{ data:[&quot;Android&quot;,&quot;IOS&quot;,&quot;PC&quot;,&quot;Ohter&quot;] }, yAxis:{ }, series:[{ name:'访问量', type:'line', data:[500,200,360,100] }] }; //初始化echarts实例 var myChart = echarts.init(document.getElementById('chart')); //使用制定的配置项和数据显示图表 myChart.setOption(option); &lt;/script&gt; 效果图如下： 一个简单的echarts图就生成了！","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://xubuhui.github.io/tags/web/"},{"name":"javaScript","slug":"javaScript","permalink":"https://xubuhui.github.io/tags/javaScript/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"Hexo+Valine+Leancloud快速搭建评论系统","slug":"Hexo-Valine-Leancloud快速搭建评论系统","date":"2019-07-22T05:11:15.000Z","updated":"2020-04-26T07:48:08.104Z","comments":true,"path":"2019/07/22/Hexo-Valine-Leancloud快速搭建评论系统/","link":"","permalink":"https://xubuhui.github.io/2019/07/22/Hexo-Valine-Leancloud快速搭建评论系统/","excerpt":"","text":"Valine 我知道的Hexo评论系统还存活的有以下几种： 畅言 Disqus Hypercomments valine 畅言虽然不错，但是需要备案，对一些初次搭建blog的人来说，很不友好，感觉大部分人会选择用github或者coding搭建blog。而且Disqus，Hypercomments和LiveRe都是国外的，加载速度挺慢。 所以说我最后选择用Valine来搭建我的博客系统，下面看我操作。 Leancloud初体验 首先我们需要去Leancloudhttps://leancloud.cn/上注册一个帐号，记住注册的时候尽量去国际版https://leancloud.app/注册，我用华东节点和华北节点都会提示我： Client verification not granted. 刚开始我也蒙圈了，最后用国际版重新注册解决的问题，貌似他们国际版和两个节点分家了。 注册完帐号之后，来到控制台， 创建一个新应用，应用名随便输入，如果你是个人blog就用开发版，然后点击创建。 Leancloud使用 创建完应用之后，点击一下刚创建的应用，进入应用设置-&gt;应用 key 然后记录下App ID和App Key的值，这相当于我们连接Leancloud的帐号密码。 记得要进入安全中心，设置好web安全域名，就是你blog的地址。 Valine使用 使用前最好下载最新的Valine，然后在主题下的_config.yml文件配置valine。 valine: enable: true app_id: 你的 app_id app_key: 你的 app_key 重新发布网站，然后稍等几分钟,大功告成，恭喜你的blog也可以收到评论了！","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://xubuhui.github.io/tags/hexo/"},{"name":"web","slug":"web","permalink":"https://xubuhui.github.io/tags/web/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"html运行exe文件，兼容浏览器","slug":"html运行exe文件，兼容浏览器","date":"2019-07-21T02:07:35.000Z","updated":"2020-04-28T05:50:20.197Z","comments":true,"path":"2019/07/21/html运行exe文件，兼容浏览器/","link":"","permalink":"https://xubuhui.github.io/2019/07/21/html运行exe文件，兼容浏览器/","excerpt":"","text":"这种方式是通过修改注册表打开本地的exe可执行文件。 第一步 打开记事本，输入下列神秘代码 Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\Typora] //Typora 协议名称 @=&quot;Typora Protocol&quot; //Typora 协议路径 &quot;URL Protocol&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\\Typora\\DefaultIcon] //Typora 协议名称 @=&quot;E:\\\\Typora\\\\Typora.exe\\&quot; //-必须修改-协议打开运行程序绝对路径地址 [HKEY_CLASSES_ROOT\\Typora\\shell] //Typora 协议名称 @=&quot;&quot; [HKEY_CLASSES_ROOT\\Typora\\shell\\open] //Typora 协议名称 @=&quot;&quot; [HKEY_CLASSES_ROOT\\Typora\\shell\\open\\command] //Typora 协议名称 @=&quot;\\&quot;E:\\\\Typora\\\\Typora.exe\\&quot; &quot; //-必须修改-协议打开运行程序绝对路径地址 除了我标注--必须修改--的路径，其余地方可以不修改。 第二步 然后另存为后缀名为reg的文件，双击运行会有个提示信息，点击确认后自动生成Typora协议。 第三步 在html文件主要改一下a标签的href路径。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;运行本地可执行文件&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;Typora://&quot; &gt;运行&lt;/a&gt; &lt;!--点击运行 （Typora协议名称）--&gt; &lt;/body&gt; &lt;/html&gt; ok，工作完成。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://xubuhui.github.io/tags/web/"},{"name":"html","slug":"html","permalink":"https://xubuhui.github.io/tags/html/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.github.io/categories/技术/"}]},{"title":"我来说点什么吧0.0","slug":"我来说点什么吧0.0","date":"2019-07-19T08:12:35.000Z","updated":"2020-04-17T07:08:38.271Z","comments":true,"path":"2019/07/19/我来说点什么吧0.0/","link":"","permalink":"https://xubuhui.github.io/2019/07/19/我来说点什么吧0.0/","excerpt":"","text":"怎么说呢，我之前真的好懒呀，身为程序猿，连一个属于自己的正二八经的blog都没有，github也是刚捡起来（我可是真的懒0.0），从今天起，我要从这个小blog中记录工作学习中遇到的问题，也会时不时分享一些心得感悟啥的，大家也可以多了解下一个中二的小哥哥。 其实CSDN也断断续续发过一些文章，不过没几个人看了，就当记录自己遇到的问题了，如果有机会，我会把之前CSDN写的文章搬过来。 那么爆一下博主中二的照片吧… 最后还是要谢谢，白猫，hojun大神提供的主题！","categories":[{"name":"随想","slug":"随想","permalink":"https://xubuhui.github.io/categories/随想/"}],"tags":[{"name":"介绍","slug":"介绍","permalink":"https://xubuhui.github.io/tags/介绍/"},{"name":"小记","slug":"小记","permalink":"https://xubuhui.github.io/tags/小记/"}],"keywords":[{"name":"随想","slug":"随想","permalink":"https://xubuhui.github.io/categories/随想/"}]},{"title":"一个不断补充的2020","slug":"一个不断补充的2020","date":"1970-01-01T00:00:02.020Z","updated":"2020-04-30T06:56:28.624Z","comments":true,"path":"1970/01/01/一个不断补充的2020/","link":"","permalink":"https://xubuhui.github.io/1970/01/01/一个不断补充的2020/","excerpt":"","text":"🎈初衷 2020年确实非常特别，新冠病毒病毒的爆发使得国内所有学校停学，说来惭愧，从20年1月中旬回家到文章开始的日子（20年4月29日），我已经在家宅了1个季度之多，回想前三个月，大多在与游戏作伴，脑海算是一片空白。 😪我真的虚度了太多时间。 悟以往之不谏,知来者之可追 总归是要行动起来的。 我准备从现在开始记录2020，希望剩下的每一天都留下一些有价值的回忆。 🎨四月 🙄4.29 攒了很久的海贼，今天准备开始慢慢看了，我看动漫一般不喜欢看漫画，所以我的进度还挺慢的。 忙活了一上午，把及时更新的图床搭建起来了。 CSDN也开始佛性发，先做积累，微信公众号先搁置。 服务器 域名 备案。 下午用Netlify加速了github pages（买了个腾讯云的cdn体验包1元，🙃嘻嘻没用上）。 晚上跟发小一起走了5公里。 😐4.30 四月的最后一天，四月是你的谎言（暗示）。 学习python爬虫的IP代理池。 潍坊android app二开。 开始有在B站做视频的想法，不过还没具体头绪 🎄五月","categories":[{"name":"随想","slug":"随想","permalink":"https://xubuhui.github.io/categories/随想/"}],"tags":[],"keywords":[{"name":"随想","slug":"随想","permalink":"https://xubuhui.github.io/categories/随想/"}]}]}