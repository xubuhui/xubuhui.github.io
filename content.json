{"meta":{"title":"HeZ","subtitle":null,"description":"纵有千古，横有八荒；前途似海，来日方长。","author":"HeZ","url":"https://xubuhui.coding.me"},"pages":[{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"comment/index.html","permalink":"https://xubuhui.coding.me/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"client/index.html","permalink":"https://xubuhui.coding.me/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2019-07-18T05:52:03.778Z","comments":false,"path":"about/index.html","permalink":"https://xubuhui.coding.me/about/index.html","excerpt":"","text":"[HeZ] 与&nbsp; HeZ （ 悔やまないことを許す ） 对话中... function bot_ui_ini() { var botui = new BotUI(\"hello-mashiro\"); botui.message.add({ delay: 800, content: \"Hi！ 👋\" }).then(function () { botui.message.add({ delay: 1100, content: \"这里是 HeZ\" }).then(function () { botui.message.add({ delay: 1100, content: \"一个可爱的蓝孩子~\" }).then(function () { botui.action.button({ delay: 1600, action: [{ text: \"然后呢？ 😃\", value: \"sure\" }, { text: \"少废话！ 🙄\", value: \"skip\" }] }).then(function (a) { \"sure\" == a.value && sure(); \"skip\" == a.value && end() }) }) }) }); var sure = function () { botui.message.add({ delay: 600, content: \"😘\" }).then(function () { secondpart() }) }, end = function () { botui.message.add({ delay: 600, content: \"![...](https://view.moezx.cc/images/2018/05/06/a1c4cd0452528b572af37952489372b6.md.jpg)\" }) }, secondpart = function () { botui.message.add({ delay: 1500, content: \"目前就读于山东某所神秘学校\" }).then(function () { botui.message.add({ delay: 1500, content: \"向往技术，也喜欢游戏\" }).then(function () { botui.message.add({ delay: 1200, content: \"偶尔会变成中二少年\" }).then(function () { botui.message.add({ delay: 1500, content: \"主攻 php 语言和 Python，偶尔也折腾 HTML/CSS/JavaScript/Java\" }).then(function () { botui.message.add({ delay: 1500, content: \"毕业想去深圳发展，实现王总的小目标\" }).then(function () { botui.message.add({ delay: 1800, content: \"喜欢lol,年轻时候想过去打职业，现在...只能玩玩匹配这样子了\" }).then(function () { botui.action.button({ delay: 1100, action: [{ text: \"为什么叫HeZ呢？ 🤔\", value: \"why-HeZ\" }] }).then(function (a) { thirdpart() }) }) }) }) }) }) }) }, thirdpart = function () { botui.message.add({ delay: 1E3, content: \"HeZ...emmmmm，频率单位嘛，主要是觉得叫起来酷酷的~\" }).then(function () { botui.action.button({ delay: 1500, action: [{ text: \"为什么是许不悔呢？ 🤔\", value: \"why-Buhui Xu\" }] }).then(function (a) { fourthpart() }) }) }, fourthpart = function () { botui.message.add({ delay: 1E3, content: \"因为希望自己做的每一个决定都不后悔 \" }).then(function () { botui.message.add({ delay: 1100, content: \"而且我希望自己以后能直面生活，勇往直前\" }).then(function () { botui.action.button({ delay: 1500, action: [{ text: \"域名有什么含意吗？(ง •_•)ง\", value: \"why-domain\" }] }).then(function (a) { fifthpart() }) }) }) }, fifthpart = function () { botui.message.add({ delay: 1E3, content: \"域名还没加上呢，慢慢的我会加上的。\" }).then(function () { botui.message.add({ delay: 1600, content: \"那么，仔细看看我的博客吧？ ^_^\" }) }) } } bot_ui_ini()","keywords":"关于"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2019-07-18T05:50:46.250Z","comments":false,"path":"donate/index.html","permalink":"https://xubuhui.coding.me/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了ya~"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2019-07-18T08:41:33.713Z","comments":false,"path":"lab/index.html","permalink":"https://xubuhui.coding.me/lab/index.html","excerpt":"","text":"sakura主题谢谢Mashiro大神辛苦制作的主题！","keywords":"Lab实验室"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2019-07-20T09:31:47.398Z","comments":false,"path":"music/index.html","permalink":"https://xubuhui.coding.me/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"rss/index.html","permalink":"https://xubuhui.coding.me/rss/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2019-05-31T23:21:46.000Z","comments":true,"path":"tags/index.html","permalink":"https://xubuhui.coding.me/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://xubuhui.coding.me/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"video/index.html","permalink":"https://xubuhui.coding.me/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2019-05-31T23:21:46.000Z","comments":false,"path":"bangumi/index.html","permalink":"https://xubuhui.coding.me/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"links","date":"2019-08-05T12:11:06.000Z","updated":"2019-08-07T02:32:28.433Z","comments":true,"path":"links/index.html","permalink":"https://xubuhui.coding.me/links/index.html","excerpt":"","text":"","keywords":"友人帐"}],"posts":[{"title":"","slug":"正则表达式","date":"2020-04-27T12:04:38.736Z","updated":"2020-04-27T12:50:58.824Z","comments":true,"path":"2020/04/27/正则表达式/","link":"","permalink":"https://xubuhui.coding.me/2020/04/27/正则表达式/","excerpt":"","text":"正则表达式","categories":[],"tags":[],"keywords":[]},{"title":"python爬取图片（二）-海贼王高清壁纸","slug":"python爬取图片（二）-海贼王高清壁纸","date":"2020-04-26T04:16:30.000Z","updated":"2020-04-26T07:53:53.187Z","comments":true,"path":"2020/04/26/python爬取图片（二）-海贼王高清壁纸/","link":"","permalink":"https://xubuhui.coding.me/2020/04/26/python爬取图片（二）-海贼王高清壁纸/","excerpt":"","text":"网页分析海贼王高清壁纸 代码我们主要是用的etree模块获取网页元素 谷歌浏览器，f12检查发现 目标后缀路径获取 //ul[@class=”pic-list2 clearfix”]//a[@class=”pic”]/@href 在Elements栏直接ctrl+f输入上列代码可以定位到符合位置 这说明图片这里的图片只是一个展示性的图片，点进图片后进入图片集 继续分析，发现每一个图片都包裹在这里 图片路径获取 //img[@id=”bigImg”]/@src 下一张图片路径获取 ​ //a[@id=”pageNext”]/@href 当然路径获取的是一个列表，如果需要获取字符串，直接取下标为0的元素 具体代码实现import requests from lxml import etree # 图片下载 def download_img(img_url, headers, num): img_content = requests.get(img_url, headers).content # 文件路径名 img_url[-15:]截取字符串 # 提前在项目根目录建好onePiece_img文件夹 file_name = &#39;onePiece_img/&#39; + str(num) + &#39;_&#39; + img_url[-15:] with open(file_name, &#39;wb&#39;) as img: img.write(img_content) print(&quot;正在下载%s&quot; % file_name) # 图片解析 def parses_img(targets_url, headers, num, page): url = &#39;http://desk.zol.com.cn/&#39; + targets_url response = requests.get(url, headers) html = etree.HTML(response.text) # 捕获异常 try: # list提取 img_url = html.xpath(&#39;//img[@id=&quot;bigImg&quot;]/@src&#39;)[0] download_img(img_url, headers, num) img_next = html.xpath(&#39;//a[@id=&quot;pageNext&quot;]/@href&#39;)[0] parses_img(img_next, headers, num, page) except: print(&quot;第%s页第%s组结束&quot; % (page, num)) # 获取url def get_url(): # 爬两页的图片 for i in range(2): if i == 0: url = &#39;http://desk.zol.com.cn/dongman/haizeiwang/&#39; else: url = &#39;http://desk.zol.com.cn/dongman/haizeiwang/&#39; + str(i + 1) + &#39;.html&#39; page = i + 1 # 浏览器标识 headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&quot; } response = requests.get(url, headers) html = etree.HTML(response.text) targets_url = html.xpath(&#39;//ul[@class=&quot;pic-list2 clearfix&quot;]//a[@class=&quot;pic&quot;]/@href&#39;) num = 1 for url in targets_url: parses_img(url, headers, num, page) num = num + 1 get_url()","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.coding.me/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.coding.me/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"python爬取百度疫情数据(三) - pyecharts疫情分布地图","slug":"python爬取百度疫情数据(三) - pyecharts疫情分布地图","date":"2020-04-25T10:15:30.000Z","updated":"2020-04-26T07:52:15.126Z","comments":true,"path":"2020/04/25/python爬取百度疫情数据(三) - pyecharts疫情分布地图/","link":"","permalink":"https://xubuhui.coding.me/2020/04/25/python爬取百度疫情数据(三) - pyecharts疫情分布地图/","excerpt":"","text":"前言这一篇综合从百度实时疫情网址爬取数据并用pyecharts显示疫情地图分布。 其中地图分为具体省份疫情地图和中国总体疫情地图。 每一个省份对应一个html地图。 pyecharts map模块链接 实现思路是 获取数据 保存为html.txt文件 解析html.txt，获取需要的数据，并保存为data.json 分别创建省份疫情地图和中国疫情的方法，填充数据 运行main.py 遇到的问题在map_draw.py中，其中具体省份疫情地图是用的pyecharts官网广东地图实例，其中有一个参数Faker.guangdong_city，我们把demo复制到pycharm ctrl+鼠标左键 进入这个参数后， 发现这个参数的数据是这样的： guangdong_city = [“汕头市”, “汕尾市”, “揭阳市”, “阳江市”, “肇庆市”, “广州市”, “惠州市”] 而我们获取的数据有些是不带市的，所以要进行一下字符串的处理 for each_city in each[&#39;subList&#39;]: city.append(each_city[&#39;city&#39;] + &quot;市&quot;) confirmeds.append(each_city[&#39;confirmed&#39;]) map.to_map_city(city, confirmeds, province, update_tqime) if province == &#39;上海&#39; or &#39;北京&#39; or &#39;天津&#39; or &#39;重庆&#39;: for each_city in each[&#39;subList&#39;]: city.append(each_city[&#39;city&#39;]) confirmeds.append(each_city[&#39;confirmed&#39;]) map.to_map_city(city, confirmeds, province, update_time) 注意需要提前在根目录建好map文件夹，存储具体省份的疫情图。 最终效果中国总体累计确诊分布图： 广东省累计确诊分布图： 具体代码实现data_get.py: import requests from lxml import etree import json import re # 获取数据 class Get_data(): # 抓取数据 def get_data(self): url = &#39;https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_1&#39; response = requests.get(url) with open(&#39;html.txt&#39;,&#39;w&#39;) as file: file.write(response.text) print(&quot;写入txt成功&quot;) # 获得更新时间 def get_time(self): with open(&#39;html.txt&#39;, &#39;r&#39;) as file: text = file.read() time = re.findall(&#39;&quot;mapLastUpdatedTime&quot;:&quot;(.*?)&quot;&#39;, text)[0] return time # 解析数据 def parse_data(self): with open(&#39;html.txt&#39;, &#39;r&#39;) as file: text = file.read() html = etree.HTML(text) result = html.xpath(&#39;//script[@type=&quot;application/json&quot;]/text()&#39;)[0] result = json.loads(result) result = result[&#39;component&#39;][0][&#39;caseList&#39;] # result[&#39;component&#39;][0]获得列表第0项 是一个字典 # print(result[&#39;component&#39;][0][&#39;caseList&#39;]) # 字典转换为字符串 result = json.dumps(result) with open(&#39;data.json&#39;, &#39;w&#39;) as file: file.write(result) print(&#39;写入json成功&#39;) # data = Get_data() # data.get_data() # data.get_time() # data.parse_data() data_more.py import json import map_draw import data_get with open(&#39;data.json&#39;, &#39;r&#39;) as file: data = file.read() data = json.loads(data) map = map_draw.Draw_map() # print(data) # 中国疫情地图数据 def china_map(update_time): area = [] confirmed = [] for each in data: # print(each) area.append(each[&#39;area&#39;]) confirmed.append(each[&#39;confirmed&#39;]) map.to_map_china(area, confirmed, update_time) # 省份疫情地图数据 def province_map(update_time): for each in data: city = [] confirmeds = [] province = each[&#39;area&#39;] # 出现空列表是因为特别行政区 for each_city in each[&#39;subList&#39;]: city.append(each_city[&#39;city&#39;] + &quot;市&quot;) confirmeds.append(each_city[&#39;confirmed&#39;]) map.to_map_city(city, confirmeds, province, update_time) if province == &#39;上海&#39; or &#39;北京&#39; or &#39;天津&#39; or &#39;重庆&#39;: for each_city in each[&#39;subList&#39;]: city.append(each_city[&#39;city&#39;]) confirmeds.append(each_city[&#39;confirmed&#39;]) map.to_map_city(city, confirmeds, province, update_time) print(&quot;生成城市疫情地图成功&quot;) map_draw.py from pyecharts import options as opts from pyecharts.charts import Map from pyecharts.faker import Faker class Draw_map(): # 颜色RGB转换 脱离函数，不需要定义self def get_color(self,a,b,c): result = &#39;#&#39;+&#39;&#39;.join(map((lambda x:&quot;%02x&quot; % x),(a,b,c))) return result.upper() def to_map_city(self, area, variate,province,update_time): pieces = [ {&quot;max&quot;: 99999999, &quot;min&quot;: 1001, &#39;label&#39;: &#39;&gt;10000&#39;, &#39;color&#39;: self.get_color(102,2,8)}, {&quot;max&quot;: 9999, &quot;min&quot;: 1000, &#39;label&#39;: &#39;1000-9999&#39;, &#39;color&#39;:self.get_color(140,13,13)}, {&quot;max&quot;: 999, &quot;min&quot;: 500, &#39;label&#39;: &#39;500-999&#39;, &#39;color&#39;:self.get_color(204,41,41)}, {&quot;max&quot;: 499, &quot;min&quot;: 100, &#39;label&#39;: &#39;100-999&#39;, &#39;color&#39;: self.get_color(255,123,105)}, {&quot;max&quot;: 99, &quot;min&quot;: 50, &#39;label&#39;: &#39;50-99&#39;, &#39;color&#39;: self.get_color(255,170,133)}, {&quot;max&quot;: 49, &quot;min&quot;: 10, &#39;label&#39;: &#39;10-49&#39;, &#39;color&#39;:self.get_color(255,202,179)}, {&quot;max&quot;: 9, &quot;min&quot;: 1, &#39;label&#39;: &#39;1-9&#39;, &#39;color&#39;: self.get_color(255,228,217)}, {&quot;max&quot;: 0, &quot;min&quot;: 0, &#39;label&#39;: &#39;0&#39;, &#39;color&#39;: self.get_color(255,255,255)}, ] c = ( # 设置地图大小 Map(init_opts=opts.InitOpts(width=&#39;1000px&#39;,height=&#39;880px&#39;)) .add(&quot;累计确诊人数&quot;, [list(z) for z in zip(area, variate)],province,is_map_symbol_show=False) .set_global_opts( title_opts=opts.TitleOpts(title=&quot;%s地区疫情地图分布&quot;%(province),subtitle=&#39;截至%s %s省疫情分布情况&#39;%(update_time,province) ,pos_left=&#39;center&#39;,pos_top=&#39;30px&#39;,), legend_opts=opts.LegendOpts(is_show=False), visualmap_opts=opts.VisualMapOpts(max_=200, is_piecewise=True, pieces=pieces) ) .render(&quot;./map/{}疫情地图.html&quot;.format(province)) ) def to_map_china(self, area, variate, update_time): # 分段 pieces = [ {&quot;max&quot;: 99999999, &quot;min&quot;: 1001, &#39;label&#39;: &#39;&gt;10000&#39;, &#39;color&#39;: &#39;#8A0808&#39;}, {&quot;max&quot;: 9999, &quot;min&quot;: 1000, &#39;label&#39;: &#39;1000-9999&#39;, &#39;color&#39;: &#39;#B40404&#39;}, {&quot;max&quot;: 999, &quot;min&quot;: 100, &#39;label&#39;: &#39;100-999&#39;, &#39;color&#39;: &#39;#DF0101&#39;}, {&quot;max&quot;: 99, &quot;min&quot;: 10, &#39;label&#39;: &#39;1-9&#39;, &#39;color&#39;: &#39;#F5A9A9&#39;}, {&quot;max&quot;: 9, &quot;min&quot;: 1, &#39;label&#39;: &#39;1-9&#39;, &#39;color&#39;: &#39;#F5A9A9&#39;}, {&quot;max&quot;: 0, &quot;min&quot;: 0, &#39;label&#39;: &#39;0&#39;, &#39;color&#39;: &#39;#FFFFFF&#39;}, ] c = ( Map(init_opts=opts.InitOpts(width=&#39;1000px&#39;, height=&#39;880px&#39;)) # zip内置函数，实现数据对应 .add(&quot;累计确诊人数&quot;, [list(z) for z in zip(area, variate)], &quot;china&quot;) .set_global_opts( title_opts=opts.TitleOpts(title=&quot;中国疫情地图分布&quot;, subtitle=&#39;截至%s 中国疫情分布情况&#39; % (update_time), pos_left=&#39;center&#39;,pos_top=&#39;30px&#39;), visualmap_opts=opts.VisualMapOpts(max_=200, is_piecewise=True, pieces=pieces), ) .render(&quot;中国疫情地图.html&quot;) ) print(&quot;生成中国疫情地图成功&quot;) main.py import data_more import data_get datas = data_get.Get_data() # 爬取数据并保存 datas.get_data() # 更新时间 update_time = datas.get_time() # 解析数据 datas.parse_data() # 生成中国疫情地图 data_more.china_map(update_time) # 生成城市疫情地图 data_more.province_map(update_time)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.coding.me/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.coding.me/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"python爬取百度疫情数据(二) - wordcloud词云","slug":"python爬取百度疫情数据(二) - wordcloud词云","date":"2020-04-24T02:10:30.000Z","updated":"2020-04-26T07:49:27.796Z","comments":true,"path":"2020/04/24/python爬取百度疫情数据(二) - wordcloud词云/","link":"","permalink":"https://xubuhui.coding.me/2020/04/24/python爬取百度疫情数据(二) - wordcloud词云/","excerpt":"","text":"前言数据用的是上一篇我们从百度实时疫情数据扒下来的，具体数据获取方式见本blog上一篇。 我们主要获取中国和世界的累计确诊人数分别制作词云,词越大表示确诊人数越多. 准备工作代码主要用到两个模块: openpyxl WordCloud wordcloud 库把词云当作一个WordCloud对象 wordcloud.WordCloud()代表一个文本对应的词云 可以根据文本中词语出现的频率等参数绘制词云 绘制词云的形状、尺寸和颜色均可设定 以WordCloud对象为基础，配置参数、加载文本、输出文件 参数 描述 width 指定词云对象生成图片的宽度,默认400像素 height 指定词云对象生成图片的高度,默认200像素 min_font_size 指定词云中字体的最小字号，默认4号 font_step 指定词云中字体字号的步进间隔，默认为1 font_path 指定文体文件的路径，默认None，解决中文乱码 max_words 指定词云显示的最大单词数量,默认200 stop_words 指定词云的排除词列表，即不显示的单词列表 mask 指定词云形状，默认为长方形，需要引用imread()函数 background_color 指定词云图片的背景颜色，默认为黑色 实现效果图中国疫情词云 世界疫情词云 具体代码实现import openpyxl from wordcloud import WordCloud # 读取数据 wb = openpyxl.load_workbook(&#39;data.xlsx&#39;) # 获取工作表 ws =wb[&#39;国内疫情&#39;] # 国内疫情数据 frequency_in = {} for row in ws.values: # 去除表头 if row[0] == &#39;省份&#39;: pass else: # 将省份和累计确诊数关联 float转换为浮点型 frequency_in[row[0]] = float(row[1]) # 海外疫情数据 frequency_out = {} # 获取表名 sheet_name = wb.sheetnames for each in sheet_name: if &#39;洲&#39; in each: ws = wb[each] for row in ws.values: if row[0] ==&#39;国家&#39;: pass else: # 写入字典 frequency_out[row[0]] = float(row[1]) # 创建生成词云方法 def generate_pic(frequency, name): # font_path 字体路径 解决不显示中文 wordcloud = WordCloud(font_path=&#39;msyh.ttc&#39;, width=1920, height=1080) # 根据确诊病例生成词云 wordcloud.generate_from_frequencies(frequency) wordcloud.to_file(&#39;%s.png&#39;%(name)) print(&quot;ok!&quot;) generate_pic(frequency_in, &quot;国内疫情词云&quot;) generate_pic(frequency_out, &quot;世界疫情词云&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.coding.me/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.coding.me/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"python爬取百度疫情数据(一)","slug":"python爬取百度疫情数据(一)","date":"2020-04-23T12:10:30.000Z","updated":"2020-04-26T07:48:43.531Z","comments":true,"path":"2020/04/23/python爬取百度疫情数据(一)/","link":"","permalink":"https://xubuhui.coding.me/2020/04/23/python爬取百度疫情数据(一)/","excerpt":"","text":"前言百度疫情和腾讯疫情数据数据爬取方式不同，腾讯疫情数据把数据放在数据包里，而百度则把数据包裹在script标签内。 百度实时疫情网址 准备工作编辑器还是推荐 PyCharm下载链接 本篇需要导入的python模块 import jsonimport requestsfrom lxml import etreeimport openpyxl json —— 老朋友了，这个模块主要是解码编码，以及格式转换 requests——灵活的爬虫库，requests请求的底层实现其实就是urllib3，不要再用urllib2做爬虫了，兄弟萌。 etree——主要是为了从html源码中得到自己想要的内容。 openpyxl——处理excel文件 抓包分析我感觉做爬虫最重要的还是数据来源的分析，因为每一个网站数据来源都有可能不一样，就像百度和腾讯做的疫情实时数据，不得不说，百度这一块做得确实比腾讯有过之而无不及。 首先进入网站，我一开始也是老三样，F12-&gt;Network-&gt;XHR，发现只有一个数据包，但是并没有我们想要的数据。 然后准备从网页源码入手，复制一个数据查找后，发现数据被包裹在一个script标签内，然后准备先把script标签内的数据扒下来。 分析数据的时候要跟原网页数据对应，确定数据的中文意思。 国内数据在caseList内 国内数据对应参数 省份——area 累计确诊——confirmed 死亡——died 治愈——crued 现有确诊——curConfirm 累计确诊增量——confirmedRelative 海外数据在globalList内 海外数据对应参数 洲——area 国家——country 累计确诊——confirmed 死亡——died 治愈——crued 现有确诊——curConfirm 累计确诊增量——confirmedRelative 具体代码实现import json import requests from lxml import etree import openpyxl url = &#39;https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_1&#39; response = requests.get(url) # print(response.text) # 生成html对象 html = etree.HTML(response.text) # 获取列表 r = html.xpath(&#39;//script[@type=&quot;application/json&quot;]/text()&#39;) # json.loads将字符串转换为python数据类型 result = json.loads(r[0]) # print(result[&#39;component&#39;][0][&#39;caseList&#39;]) # 创建工作簿 wb = openpyxl.Workbook() # 创建工作表 ws = wb.active ws.title = &#39;国内疫情&#39; # 表头 ws.append([&#39;省份&#39;,&#39;累计确诊&#39;,&#39;死亡&#39;,&#39;治愈&#39;,&#39;现有确诊&#39;,&#39;累计确诊增量&#39;]) # 国内疫情数据 result_in = result[&#39;component&#39;][0][&#39;caseList&#39;] for data_in in result_in: temp_list = [data_in[&#39;area&#39;], data_in[&#39;confirmed&#39;],data_in[&#39;died&#39;],data_in[&#39;crued&#39;],data_in[&#39;curConfirm&#39;], data_in[&#39;confirmedRelative&#39;] ] # 判断是否为空 如果空数据则赋值 0，0是字符串 for i in range(len(temp_list)): if temp_list[i] == &#39;&#39;: temp_list[i] = &#39;0&#39; ws.append(temp_list) # 国外疫情数据 result_out = result[&#39;component&#39;][0][&#39;globalList&#39;]; # print(result_out) for data_out in result_out: # 表名 sheet_title = data_out[&#39;area&#39;] # 创建新的工作表 ws_out = wb.create_sheet(sheet_title) ws_out.append([&#39;国家&#39;,&#39;累计确诊&#39;,&#39;死亡&#39;,&#39;治愈&#39;,&#39;现有确诊&#39;,&#39;累计确诊增量&#39;]) # subList 具体国家疫情数据 for sub_out in data_out[&#39;subList&#39;]: sub_list = [sub_out[&#39;country&#39;], sub_out[&#39;confirmed&#39;],sub_out[&#39;died&#39;],sub_out[&#39;crued&#39;],sub_out[&#39;curConfirm&#39;], sub_out[&#39;confirmedRelative&#39;]] # 防止数据为空 如果空则 赋值0 for i in range(len(sub_list)): if sub_list[i] == &#39;&#39;: sub_list[i] = &#39;0&#39; ws_out.append(sub_list) wb.save(&#39;./data.xlsx&#39;) print(&quot;Ok&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.coding.me/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.coding.me/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"python爬取腾讯疫情数据并可视化（二）","slug":"python爬取腾讯疫情数据并可视化（二）","date":"2020-04-22T13:50:30.000Z","updated":"2020-04-26T07:51:19.744Z","comments":true,"path":"2020/04/22/python爬取腾讯疫情数据并可视化（二）/","link":"","permalink":"https://xubuhui.coding.me/2020/04/22/python爬取腾讯疫情数据并可视化（二）/","excerpt":"","text":"前言我们需要爬取的是腾讯实时疫情的数据–腾讯实时疫情网址 这篇用pyecharts的南丁格尔(玫瑰图)实现数据可视化。 准备工作本篇需要pyecharts库，安装方式如下： 例：安装方式-&gt;CMD命令行-&gt;安装requests pip install requests 这样安装的pyechatrs V1为最新版，与pyecharts–0.5不兼容 我们的数据为 最终实现效果 下面是具体代码实现import pandas as pd from pyecharts.charts import Pie from pyecharts import options as opts # 读入数据, nrows数据行数 df = pd.read_csv(&#39;data.csv&#39;,nrows=30) # 降序操作 df.sort_values(by=&#39;确诊病例&#39;, ascending=False, inplace=True) # 提取数据 area = df[&#39;国家和地区&#39;].values.tolist() num = df[&#39;确诊病例&#39;].values.tolist() # 自定义颜色 color_series = [ &#39;#D02C2A&#39;, &#39;#D44C2D&#39;, &#39;#F57A34&#39;, &#39;#FA8F2F&#39;, &#39;#D99D21&#39;, &#39;#CF7B25&#39;, &#39;#CF7B25&#39;, &#39;#CF7B25&#39;,&#39;#FAE927&#39;, &#39;#E9E416&#39;, &#39;#C9DA36&#39;, &#39;#9ECB3C&#39;, &#39;#6DBC49&#39;, &#39;#37B44E&#39;, &#39;#3DBA78&#39;, &#39;#14ADCF&#39;, &#39;#209AC9&#39;, &#39;#1E91CA&#39;, &#39;#2C6BA0&#39;, &#39;#2B55A1&#39;, &#39;#2D3D8E&#39;, &#39;#44388E&#39;, &#39;#6A368B&#39; &#39;#7D3990&#39;, &#39;#A63F98&#39;, &#39;#C31C88&#39;, &#39;#D52178&#39;, &#39;#D5225B&#39;, ] # 实例化Pie类 pie1 = Pie(init_opts=opts.InitOpts(width=&#39;1350px&#39;, height=&#39;700px&#39;)) # 设置颜色 pie1.set_colors(color_series) # 添加数据，设置饼图的半径，是否展示成南丁格尔图 pie1.add(&quot;&quot;, [(a,b) for a,b in zip(area, num)], radius=[&quot;40%&quot;, &quot;100%&quot;], center=[&quot;50%&quot;, &quot;65%&quot;], rosetype=&quot;radius&quot; ) # 设置全局配置项 pie1.set_global_opts(title_opts=opts.TitleOpts(title=&#39;南丁格尔疫情图&#39;), legend_opts=opts.LegendOpts(is_show=False), ) # 设置系列配置项 pie1.set_series_opts(label_opts=opts.LabelOpts(is_show=True, position=&quot;top&quot;, font_size=12, formatter=&quot;{b}:累计确诊{c}例&quot;, font_style=&quot;italic&quot;, font_weight=&quot;bold&quot;, font_family=&quot;Microsoft YaHei&quot; ), ) # 生成html文档 pie1.render(&#39;南丁格尔疫情图.html&#39;) print(&quot;构建成功&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.coding.me/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.coding.me/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"python爬取腾讯疫情数据并可视化（一）","slug":"python爬取腾讯疫情数据并可视化（一）","date":"2020-04-22T12:50:30.000Z","updated":"2020-04-26T07:51:29.476Z","comments":true,"path":"2020/04/22/python爬取腾讯疫情数据并可视化（一）/","link":"","permalink":"https://xubuhui.coding.me/2020/04/22/python爬取腾讯疫情数据并可视化（一）/","excerpt":"","text":"前言我们需要爬取的是腾讯实时疫情的数据–腾讯实时疫情网址 这里准备用两篇文章实现python爬取疫情数据，并结合pyecharts的南丁格尔(玫瑰图)实现数据可视化。 准备工作本篇需要用到三个python库, 请自行安装 requests pandas json 例：安装方式-&gt;CMD命令行-&gt;安装requests pip install requests 抓包分析我用的是谷歌浏览器，进入腾讯实时疫情的网址，然后F12-&gt;Network-&gt;XHR，重新刷新下页面发现 确定这就是数据包。 下面是具体代码实现import requests import json import pandas as pd # 分析抓包 url = &#39;https://api.inews.qq.com/newsqa/v1/automation/foreign/country/ranklist&#39; response = requests.get(url) content = json.loads(response.text) # json.loads 将json格式数据转换为字典 # print(content) df = pd.DataFrame(columns=[&#39;国家和地区&#39;,&#39;确诊病例&#39;,&#39;死亡病例&#39;,&#39;治愈病例&#39;]) # 字典类型读取到DataFrame for i in range(len(content[&#39;data&#39;])): df.loc[i+1] = [ content[&#39;data&#39;][i][&#39;name&#39;], content[&#39;data&#39;][i][&#39;confirm&#39;], content[&#39;data&#39;][i][&#39;dead&#39;], content[&#39;data&#39;][i][&#39;heal&#39;]] # 保存为xlsx格式文件 df.to_excel(&#39;data.xlsx&#39;,index=0, encoding=&#39;utf-8&#39;) print(&#39;爬取完毕&#39;)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.coding.me/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.coding.me/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"python爬取图片（一）-王者荣耀高清皮肤图","slug":"python爬取图片（一）-王者荣耀高清皮肤图","date":"2020-04-21T12:52:25.000Z","updated":"2020-04-26T07:56:28.633Z","comments":true,"path":"2020/04/21/python爬取图片（一）-王者荣耀高清皮肤图/","link":"","permalink":"https://xubuhui.coding.me/2020/04/21/python爬取图片（一）-王者荣耀高清皮肤图/","excerpt":"","text":"关于爬虫的一些思路首先我们打开王者荣耀官网，进入英雄资料界面 python爬取腾讯疫情数据并可视化（二）进入一个英雄的具体介绍页面 然后f12，打开浏览器调试窗口，首先找到皮肤图片的url 分析提供图片的数据包 NetWork-&gt;XHR 下面是具体代码实现# 抓取王者荣耀皮肤 import requests import pprint import time # 程序开始时间 start_time = time.time() # 1.分析目标网页，确定爬取的url路径 url = &#39;https://pvp.qq.com/web201605/js/herolist.json&#39; # 2.发送请求 --requests 模拟浏览器发送请求，获取相应数据 response = requests.get(url) data = response.json() # 格式化打印json # pprint.pprint(data) # 3.解析数据 --json模块:把json字符串转化成python可交互的数据类型 for data1 in data: cname = data1[&#39;cname&#39;] # 英雄名 ename = data1[&#39;ename&#39;] # 英雄ID try: skin_name = data1[&#39;skin_name&#39;].split(&#39;|&#39;) # 皮肤名称 except Exception as e: print(e) # 构建皮肤数量循环 for skin_num in range(1, len(skin_name)+1): # 拼接图片url # &#39;http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/&#39;+英雄ID值+&#39;/&#39;+英雄ID值+&#39;-bigskin-&#39;+皮肤序号+&#39;.jpg&#39; skin_url = &#39;http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/&#39;+str(ename)+&#39;/&#39;+str(ename)+&#39;-bigskin-&#39;+str(skin_num)+&#39;.jpg&#39; img_data = requests.get(skin_url).content # 二进制数据提取 # 4.保存数据 --保存在目标文件夹 with open(&#39;img\\\\&#39;+cname+&#39;-&#39;+skin_name[skin_num-1]+&#39;.jpg&#39;,&#39;wb&#39;) as f: print(&#39;正在下载图片：&#39;, cname+&#39;-&#39;+skin_name[skin_num-1]) f.write(img_data) f.close() # 程序结束时间 end_time = time.time() # 打印程序运行花费时间 print(&#39;花费时间：&#39;,end_time-start_time)","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"python","slug":"python","permalink":"https://xubuhui.coding.me/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://xubuhui.coding.me/tags/爬虫/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"php开启redis扩展","slug":"php开启redis扩展","date":"2020-04-10T07:12:30.000Z","updated":"2020-04-26T07:47:21.210Z","comments":true,"path":"2020/04/10/php开启redis扩展/","link":"","permalink":"https://xubuhui.coding.me/2020/04/10/php开启redis扩展/","excerpt":"","text":"1.安装redis github下载链接 2.测试redis进入redis目录，双击运行 redis-server.exe 文件。 或者在redis安装目录，运行cmd命令行，执行 redis-server.exe redis.windows.conf 3.安装redis扩展redis扩展下载链接 然后根据phpinfo()的信息选择合适的redis扩展压缩包。 注意php版本号以及 Architecture ：CPU架构 4.将redis扩展包的php_redis.dll和php_redis.pdb两个文件放在php的ext文件夹5.打开php.ini文件，打开扩展 extension=php_redis.dll 6.php连接并测试redis数据库(先开启redis服务-redis-server)首先新建test.php &lt;?php $redis = new Redis(); $redis-&gt;connect(&#39;127.0.0.1&#39;,6379); $redis-&gt;set(&#39;name&#39;,&#39;HeZ&#39;); echo $redis-&gt;get(&#39;name&#39;); 访问test.php,输出HeZ,测试通过。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"php","slug":"php","permalink":"https://xubuhui.coding.me/tags/php/"},{"name":"redis","slug":"redis","permalink":"https://xubuhui.coding.me/tags/redis/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"js上传并显示图片","slug":"js上传并显示图片","date":"2019-08-02T05:49:34.000Z","updated":"2019-09-24T00:34:45.308Z","comments":true,"path":"2019/08/02/js上传并显示图片/","link":"","permalink":"https://xubuhui.coding.me/2019/08/02/js上传并显示图片/","excerpt":"","text":"首先看下upload.html文件&lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;上传图片&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;css/up.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;divcss&quot;&gt; &lt;div class=&quot;portrait&quot;&gt; &lt;img src=&quot;img/avatar.png&quot; alt=&quot;&quot; class=&quot;avatar&quot; id=&quot;imgupload&quot;&gt; &lt;p&gt;头像&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;file&quot;&gt; 点击上传 &lt;input type=&quot;file&quot; id=&quot;profile_pic&quot; &gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;script src=&quot;https://code.jquery.com/jquery-3.1.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; //上传个人头像 $(&quot;#profile_pic&quot;).change(function (e) { console.log(e); var file = e.target.files[0] || e.dataTransfer.files[0]; if (file) { //............. console.log(file); var file = document.getElementById(&quot;profile_pic&quot;).files[0]; var formData = new FormData(); formData.append(&#39;file&#39;, file); //为ajax的data做准备 var reader = new FileReader(); reader.onload = function () { $(&quot;#imgupload&quot;).attr(&quot;src&quot;, this.result); }; reader.readAsDataURL(file); $.ajax({ url: url + &quot;api/uploadOss/uploadFile&quot;,//这是你的后端接口地址 type: &quot;post&quot;,//ajax请求类型 data: formData,//存放图片资源 contentType: false, processData: false, mimeType: &quot;multipart/form-data&quot;, success: function (data) { //成功返回信息 alert(&quot;上传成功&quot;); }, error: function (data) { //失败返回信息 alert(&quot;上传头像失败&quot;); } }); } }); &lt;/script&gt; &lt;/html&gt;其次看下up.css文件body {background-image:url(&#39;../img/point.png&#39;);} .divcss { margin: 0 auto; width: 400px; height: 100px; text-align: center } .avatar { width: 350px; height: 400px } .file { position: relative; display: inline-block; background: #D0EEFF; border: 1px solid #99D3F5; border-radius: 4px; padding: 4px 12px; overflow: hidden; color: #1E88C7; text-decoration: none; text-indent: 0; line-height: 20px; } .file input { position: absolute; font-size: 100px; right: 0; top: 0; opacity: 0; } .file:hover { background: #AADFFD; border-color: #78C3F3; color: #004974; text-decoration: none; }其中有几处用到了图片背景，这个你可以根据喜好选择图片，当然也可以不用。 我的目录结构是这样子的： 最终的效果图是这样的： 码完收工。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://xubuhui.coding.me/tags/web/"},{"name":"javaScript","slug":"javaScript","permalink":"https://xubuhui.coding.me/tags/javaScript/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"超简单！js使用echarts","slug":"超简单！js使用echarts","date":"2019-07-24T04:27:54.000Z","updated":"2020-04-26T07:53:04.132Z","comments":true,"path":"2019/07/24/超简单！js使用echarts/","link":"","permalink":"https://xubuhui.coding.me/2019/07/24/超简单！js使用echarts/","excerpt":"","text":"echartsEcharts的官网https://echarts.baidu.com/,Echarts官网有很多demo可以供我们使用，这一点可以说是很良心了，不说废话了直接进入主题。 第一步首先在确定安装好node.js的前提下，在项目目录cmd命令行下输入 npm install echarts 安装echarts依赖。 第二步把图中所示的echart.js文件复制到和你的项目文件同级目录下再引入,当然你也可以直接引入，我主要是为了方便。 第三步新建html文件，输入代码 &lt;body&gt; &lt;!--创建div容器--&gt; &lt;div id=&quot;chart&quot; style=&quot;width:600px; height: 400px;&quot;&gt;&lt;/div&gt; &lt;/body&gt; &lt;!--引入charts.js--&gt; &lt;script src=&quot;echarts.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; //指定图标的配置和数据 var option = { title:{ text:&#39;ECharts 数据统计&#39; }, tooltip:{}, legend:{ data:[&#39;用户来源&#39;] }, xAxis:{ data:[&quot;Android&quot;,&quot;IOS&quot;,&quot;PC&quot;,&quot;Ohter&quot;] }, yAxis:{ }, series:[{ name:&#39;访问量&#39;, type:&#39;line&#39;, data:[500,200,360,100] }] }; //初始化echarts实例 var myChart = echarts.init(document.getElementById(&#39;chart&#39;)); //使用制定的配置项和数据显示图表 myChart.setOption(option); &lt;/script&gt;效果图如下： 一个简单的echarts图就生成了！","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://xubuhui.coding.me/tags/web/"},{"name":"javaScript","slug":"javaScript","permalink":"https://xubuhui.coding.me/tags/javaScript/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"Hexo+Valine+Leancloud快速搭建评论系统","slug":"Hexo-Valine-Leancloud快速搭建评论系统","date":"2019-07-22T05:11:15.000Z","updated":"2020-04-26T07:48:08.104Z","comments":true,"path":"2019/07/22/Hexo-Valine-Leancloud快速搭建评论系统/","link":"","permalink":"https://xubuhui.coding.me/2019/07/22/Hexo-Valine-Leancloud快速搭建评论系统/","excerpt":"","text":"Valine我知道的Hexo评论系统还存活的有以下几种： 畅言 Disqus Hypercomments valine 畅言虽然不错，但是需要备案，对一些初次搭建blog的人来说，很不友好，感觉大部分人会选择用github或者coding搭建blog。而且Disqus，Hypercomments和LiveRe都是国外的，加载速度挺慢。 所以说我最后选择用Valine来搭建我的博客系统，下面看我操作。 Leancloud初体验首先我们需要去Leancloudhttps://leancloud.cn/上注册一个帐号，记住注册的时候尽量去国际版https://leancloud.app/注册，我用华东节点和华北节点都会提示我： Client verification not granted. 刚开始我也蒙圈了，最后用国际版重新注册解决的问题，貌似他们国际版和两个节点分家了。 注册完帐号之后，来到控制台， 创建一个新应用，应用名随便输入，如果你是个人blog就用开发版，然后点击创建。 Leancloud使用创建完应用之后，点击一下刚创建的应用，进入应用设置-&gt;应用 key 然后记录下App ID和App Key的值，这相当于我们连接Leancloud的帐号密码。 记得要进入安全中心，设置好web安全域名，就是你blog的地址。 Valine使用使用前最好下载最新的Valine，然后在主题下的_config.yml文件配置valine。 valine: enable: true app_id: 你的 app_id app_key: 你的 app_key重新发布网站，然后稍等几分钟,大功告成，恭喜你的blog也可以收到评论了！","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://xubuhui.coding.me/tags/web/"},{"name":"hexo","slug":"hexo","permalink":"https://xubuhui.coding.me/tags/hexo/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"html运行exe文件，兼容浏览器","slug":"html运行exe文件，兼容浏览器","date":"2019-07-21T02:07:35.000Z","updated":"2019-07-21T17:35:04.137Z","comments":true,"path":"2019/07/21/html运行exe文件，兼容浏览器/","link":"","permalink":"https://xubuhui.coding.me/2019/07/21/html运行exe文件，兼容浏览器/","excerpt":"","text":"这种方式是通过修改注册表打开本地的exe可执行文件。 第一步打开记事本，输入下列神秘代码 Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\Typora] //Typora 协议名称 @=&quot;Typora Protocol&quot; //Typora 协议路径 &quot;URL Protocol&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\\Typora\\DefaultIcon] //Typora 协议名称 @=&quot;E:\\\\Typora\\\\Typora.exe\\&quot; //-必须修改-协议打开运行程序绝对路径地址 [HKEY_CLASSES_ROOT\\Typora\\shell] //Typora 协议名称 @=&quot;&quot; [HKEY_CLASSES_ROOT\\Typora\\shell\\open] //Typora 协议名称 @=&quot;&quot; [HKEY_CLASSES_ROOT\\Typora\\shell\\open\\command] //Typora 协议名称 @=&quot;\\&quot;E:\\\\Typora\\\\Typora.exe\\&quot; &quot; //-必须修改-协议打开运行程序绝对路径地址除了我标注--必须修改--的路径，其余地方可以不修改。 第二步然后另存为后缀名为reg的文件，双击运行会有个提示信息，点击确认后自动生成Typora协议。 第三步在html文件主要改一下a标签的href路径。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;运行本地可执行文件&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;Typora://&quot; &gt;运行&lt;/a&gt; &lt;!--点击运行 （Typora协议名称）--&gt; &lt;/body&gt; &lt;/html&gt;ok，工作完成。","categories":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://xubuhui.coding.me/tags/web/"},{"name":"html","slug":"html","permalink":"https://xubuhui.coding.me/tags/html/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://xubuhui.coding.me/categories/技术/"}]},{"title":"我来说点什么吧0.0","slug":"我来说点什么吧0.0","date":"2019-07-19T08:12:35.000Z","updated":"2020-04-17T07:08:38.271Z","comments":true,"path":"2019/07/19/我来说点什么吧0.0/","link":"","permalink":"https://xubuhui.coding.me/2019/07/19/我来说点什么吧0.0/","excerpt":"","text":"怎么说呢，我之前真的好懒呀，身为程序猿，连一个属于自己的正二八经的blog都没有，github也是刚捡起来（我可是真的懒0.0），从今天起，我要从这个小blog中记录工作学习中遇到的问题，也会时不时分享一些心得感悟啥的，大家也可以多了解下一个中二的小哥哥。 其实CSDN也断断续续发过一些文章，不过没几个人看了，就当记录自己遇到的问题了，如果有机会，我会把之前CSDN写的文章搬过来。 那么爆一下博主中二的照片吧… 最后还是要谢谢，白猫，hojun大神提供的主题！","categories":[{"name":"随想","slug":"随想","permalink":"https://xubuhui.coding.me/categories/随想/"}],"tags":[{"name":"介绍","slug":"介绍","permalink":"https://xubuhui.coding.me/tags/介绍/"},{"name":"小记","slug":"小记","permalink":"https://xubuhui.coding.me/tags/小记/"}],"keywords":[{"name":"随想","slug":"随想","permalink":"https://xubuhui.coding.me/categories/随想/"}]}]}